{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cf4fc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50a4a8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('model.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa86b1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sim_content</th>\n",
       "      <th>sin_comment</th>\n",
       "      <th>word_count</th>\n",
       "      <th>duplicate_word_ratio</th>\n",
       "      <th>no_of_sentences</th>\n",
       "      <th>length_of_comment</th>\n",
       "      <th>num_of_punctuations</th>\n",
       "      <th>is_period_sequence</th>\n",
       "      <th>stop_word_ratio</th>\n",
       "      <th>post_coment_gap</th>\n",
       "      <th>black_word_count</th>\n",
       "      <th>is_link</th>\n",
       "      <th>is_youtube_link</th>\n",
       "      <th>is_number</th>\n",
       "      <th>is_mail</th>\n",
       "      <th>comment_duplication</th>\n",
       "      <th>classifier_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.372249</td>\n",
       "      <td>0.280355</td>\n",
       "      <td>3.057751</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>3</td>\n",
       "      <td>6.612096</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.750086</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.354255</td>\n",
       "      <td>0.341604</td>\n",
       "      <td>2.410947</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>4.897796</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.380144</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.715092</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.487752</td>\n",
       "      <td>0.314966</td>\n",
       "      <td>1.578192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>3.791178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.181022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.468095</td>\n",
       "      <td>0.368193</td>\n",
       "      <td>2.328976</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5.041488</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>13.636652</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.366191</td>\n",
       "      <td>0.258061</td>\n",
       "      <td>2.238987</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5.174158</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.529500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  sim_content  sin_comment  word_count  duplicate_word_ratio  \\\n",
       "0           0     0.372249     0.280355    3.057751              0.083333   \n",
       "1           1     0.354255     0.341604    2.410947              0.000000   \n",
       "2           2     0.487752     0.314966    1.578192              0.000000   \n",
       "3           3     0.468095     0.368193    2.328976              0.000000   \n",
       "4           4     0.366191     0.258061    2.238987              0.000000   \n",
       "\n",
       "   no_of_sentences  length_of_comment  num_of_punctuations  \\\n",
       "0                3           6.612096                  4.0   \n",
       "1                1           4.897796                  1.0   \n",
       "2                1           3.791178                  0.0   \n",
       "3                1           5.041488                  0.0   \n",
       "4                1           5.174158                  4.0   \n",
       "\n",
       "   is_period_sequence  stop_word_ratio  post_coment_gap  black_word_count  \\\n",
       "0                   0         0.000000         8.750086               1.0   \n",
       "1                   0         0.000000        15.380144               2.0   \n",
       "2                   0         0.000000        14.181022               0.0   \n",
       "3                   0         0.090909        13.636652               2.0   \n",
       "4                   0         0.000000        13.529500               0.0   \n",
       "\n",
       "   is_link  is_youtube_link  is_number  is_mail  comment_duplication  \\\n",
       "0        1                0          0        0             0.000000   \n",
       "1        0                0          0        0             0.715092   \n",
       "2        0                0          0        0             0.000000   \n",
       "3        0                0          0        0             0.000000   \n",
       "4        1                1          0        0             0.000000   \n",
       "\n",
       "   classifier_val  \n",
       "0               1  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6740b5eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'sim_content', 'sin_comment', 'word_count',\n",
       "       'duplicate_word_ratio', 'no_of_sentences', 'length_of_comment',\n",
       "       'num_of_punctuations', 'is_period_sequence', 'stop_word_ratio',\n",
       "       'post_coment_gap', 'black_word_count', 'is_link', 'is_youtube_link',\n",
       "       'is_number', 'is_mail', 'comment_duplication', 'classifier_val'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rename(columns={'Unnamed: 0':'id'},inplace=True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a68913c",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90728004",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "799f4ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dfc['classifier_val']\n",
    "X = dfc.drop(['classifier_val','id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12bcfb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f56f77e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train) \n",
    "X_test = scaler.transform(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c70ae8e",
   "metadata": {},
   "source": [
    "### Handle imbalance dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "826c75aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2540\n",
       "1     585\n",
       "Name: classifier_val, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfc['classifier_val'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a726715f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\imblearn\\utils\\_validation.py:638: FutureWarning: Pass sampling_strategy=1 as keyword args. From version 0.9 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of classes before fit Counter({0: 1778, 1: 409})\n",
      "The number of classes after fit Counter({1: 1758, 0: 1758})\n"
     ]
    }
   ],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "from collections import Counter\n",
    "\n",
    "os=SMOTETomek(1)\n",
    "X_train_os,y_train_os=os.fit_sample(X_train,y_train)\n",
    "print(\"The number of classes before fit {}\".format(Counter(y_train)))\n",
    "print(\"The number of classes after fit {}\".format(Counter(y_train_os)))\n",
    "\n",
    "X_train = X_train_os\n",
    "y_train = y_train_os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0892ced",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuninng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f50513ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76118bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae1c063f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.get_params() # hyperParameter list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "672aa74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomizedSearchCV for find the most suited model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fbc713c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 100)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in range(100,2000,2)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09b09245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81d80bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': [100,\n",
       "  119,\n",
       "  138,\n",
       "  157,\n",
       "  176,\n",
       "  195,\n",
       "  215,\n",
       "  234,\n",
       "  253,\n",
       "  272,\n",
       "  291,\n",
       "  311,\n",
       "  330,\n",
       "  349,\n",
       "  368,\n",
       "  387,\n",
       "  407,\n",
       "  426,\n",
       "  445,\n",
       "  464,\n",
       "  483,\n",
       "  503,\n",
       "  522,\n",
       "  541,\n",
       "  560,\n",
       "  579,\n",
       "  598,\n",
       "  618,\n",
       "  637,\n",
       "  656,\n",
       "  675,\n",
       "  694,\n",
       "  714,\n",
       "  733,\n",
       "  752,\n",
       "  771,\n",
       "  790,\n",
       "  810,\n",
       "  829,\n",
       "  848,\n",
       "  867,\n",
       "  886,\n",
       "  906,\n",
       "  925,\n",
       "  944,\n",
       "  963,\n",
       "  982,\n",
       "  1002,\n",
       "  1021,\n",
       "  1040,\n",
       "  1059,\n",
       "  1078,\n",
       "  1097,\n",
       "  1117,\n",
       "  1136,\n",
       "  1155,\n",
       "  1174,\n",
       "  1193,\n",
       "  1213,\n",
       "  1232,\n",
       "  1251,\n",
       "  1270,\n",
       "  1289,\n",
       "  1309,\n",
       "  1328,\n",
       "  1347,\n",
       "  1366,\n",
       "  1385,\n",
       "  1405,\n",
       "  1424,\n",
       "  1443,\n",
       "  1462,\n",
       "  1481,\n",
       "  1501,\n",
       "  1520,\n",
       "  1539,\n",
       "  1558,\n",
       "  1577,\n",
       "  1596,\n",
       "  1616,\n",
       "  1635,\n",
       "  1654,\n",
       "  1673,\n",
       "  1692,\n",
       "  1712,\n",
       "  1731,\n",
       "  1750,\n",
       "  1769,\n",
       "  1788,\n",
       "  1808,\n",
       "  1827,\n",
       "  1846,\n",
       "  1865,\n",
       "  1884,\n",
       "  1904,\n",
       "  1923,\n",
       "  1942,\n",
       "  1961,\n",
       "  1980,\n",
       "  2000],\n",
       " 'max_features': ['auto', 'sqrt'],\n",
       " 'max_depth': [100,\n",
       "  102,\n",
       "  104,\n",
       "  106,\n",
       "  108,\n",
       "  110,\n",
       "  112,\n",
       "  114,\n",
       "  116,\n",
       "  118,\n",
       "  120,\n",
       "  122,\n",
       "  124,\n",
       "  126,\n",
       "  128,\n",
       "  130,\n",
       "  132,\n",
       "  134,\n",
       "  136,\n",
       "  138,\n",
       "  140,\n",
       "  142,\n",
       "  144,\n",
       "  146,\n",
       "  148,\n",
       "  150,\n",
       "  152,\n",
       "  154,\n",
       "  156,\n",
       "  158,\n",
       "  160,\n",
       "  162,\n",
       "  164,\n",
       "  166,\n",
       "  168,\n",
       "  170,\n",
       "  172,\n",
       "  174,\n",
       "  176,\n",
       "  178,\n",
       "  180,\n",
       "  182,\n",
       "  184,\n",
       "  186,\n",
       "  188,\n",
       "  190,\n",
       "  192,\n",
       "  194,\n",
       "  196,\n",
       "  198,\n",
       "  200,\n",
       "  202,\n",
       "  204,\n",
       "  206,\n",
       "  208,\n",
       "  210,\n",
       "  212,\n",
       "  214,\n",
       "  216,\n",
       "  218,\n",
       "  220,\n",
       "  222,\n",
       "  224,\n",
       "  226,\n",
       "  228,\n",
       "  230,\n",
       "  232,\n",
       "  234,\n",
       "  236,\n",
       "  238,\n",
       "  240,\n",
       "  242,\n",
       "  244,\n",
       "  246,\n",
       "  248,\n",
       "  250,\n",
       "  252,\n",
       "  254,\n",
       "  256,\n",
       "  258,\n",
       "  260,\n",
       "  262,\n",
       "  264,\n",
       "  266,\n",
       "  268,\n",
       "  270,\n",
       "  272,\n",
       "  274,\n",
       "  276,\n",
       "  278,\n",
       "  280,\n",
       "  282,\n",
       "  284,\n",
       "  286,\n",
       "  288,\n",
       "  290,\n",
       "  292,\n",
       "  294,\n",
       "  296,\n",
       "  298,\n",
       "  300,\n",
       "  302,\n",
       "  304,\n",
       "  306,\n",
       "  308,\n",
       "  310,\n",
       "  312,\n",
       "  314,\n",
       "  316,\n",
       "  318,\n",
       "  320,\n",
       "  322,\n",
       "  324,\n",
       "  326,\n",
       "  328,\n",
       "  330,\n",
       "  332,\n",
       "  334,\n",
       "  336,\n",
       "  338,\n",
       "  340,\n",
       "  342,\n",
       "  344,\n",
       "  346,\n",
       "  348,\n",
       "  350,\n",
       "  352,\n",
       "  354,\n",
       "  356,\n",
       "  358,\n",
       "  360,\n",
       "  362,\n",
       "  364,\n",
       "  366,\n",
       "  368,\n",
       "  370,\n",
       "  372,\n",
       "  374,\n",
       "  376,\n",
       "  378,\n",
       "  380,\n",
       "  382,\n",
       "  384,\n",
       "  386,\n",
       "  388,\n",
       "  390,\n",
       "  392,\n",
       "  394,\n",
       "  396,\n",
       "  398,\n",
       "  400,\n",
       "  402,\n",
       "  404,\n",
       "  406,\n",
       "  408,\n",
       "  410,\n",
       "  412,\n",
       "  414,\n",
       "  416,\n",
       "  418,\n",
       "  420,\n",
       "  422,\n",
       "  424,\n",
       "  426,\n",
       "  428,\n",
       "  430,\n",
       "  432,\n",
       "  434,\n",
       "  436,\n",
       "  438,\n",
       "  440,\n",
       "  442,\n",
       "  444,\n",
       "  446,\n",
       "  448,\n",
       "  450,\n",
       "  452,\n",
       "  454,\n",
       "  456,\n",
       "  458,\n",
       "  460,\n",
       "  462,\n",
       "  464,\n",
       "  466,\n",
       "  468,\n",
       "  470,\n",
       "  472,\n",
       "  474,\n",
       "  476,\n",
       "  478,\n",
       "  480,\n",
       "  482,\n",
       "  484,\n",
       "  486,\n",
       "  488,\n",
       "  490,\n",
       "  492,\n",
       "  494,\n",
       "  496,\n",
       "  498,\n",
       "  500,\n",
       "  502,\n",
       "  504,\n",
       "  506,\n",
       "  508,\n",
       "  510,\n",
       "  512,\n",
       "  514,\n",
       "  516,\n",
       "  518,\n",
       "  520,\n",
       "  522,\n",
       "  524,\n",
       "  526,\n",
       "  528,\n",
       "  530,\n",
       "  532,\n",
       "  534,\n",
       "  536,\n",
       "  538,\n",
       "  540,\n",
       "  542,\n",
       "  544,\n",
       "  546,\n",
       "  548,\n",
       "  550,\n",
       "  552,\n",
       "  554,\n",
       "  556,\n",
       "  558,\n",
       "  560,\n",
       "  562,\n",
       "  564,\n",
       "  566,\n",
       "  568,\n",
       "  570,\n",
       "  572,\n",
       "  574,\n",
       "  576,\n",
       "  578,\n",
       "  580,\n",
       "  582,\n",
       "  584,\n",
       "  586,\n",
       "  588,\n",
       "  590,\n",
       "  592,\n",
       "  594,\n",
       "  596,\n",
       "  598,\n",
       "  600,\n",
       "  602,\n",
       "  604,\n",
       "  606,\n",
       "  608,\n",
       "  610,\n",
       "  612,\n",
       "  614,\n",
       "  616,\n",
       "  618,\n",
       "  620,\n",
       "  622,\n",
       "  624,\n",
       "  626,\n",
       "  628,\n",
       "  630,\n",
       "  632,\n",
       "  634,\n",
       "  636,\n",
       "  638,\n",
       "  640,\n",
       "  642,\n",
       "  644,\n",
       "  646,\n",
       "  648,\n",
       "  650,\n",
       "  652,\n",
       "  654,\n",
       "  656,\n",
       "  658,\n",
       "  660,\n",
       "  662,\n",
       "  664,\n",
       "  666,\n",
       "  668,\n",
       "  670,\n",
       "  672,\n",
       "  674,\n",
       "  676,\n",
       "  678,\n",
       "  680,\n",
       "  682,\n",
       "  684,\n",
       "  686,\n",
       "  688,\n",
       "  690,\n",
       "  692,\n",
       "  694,\n",
       "  696,\n",
       "  698,\n",
       "  700,\n",
       "  702,\n",
       "  704,\n",
       "  706,\n",
       "  708,\n",
       "  710,\n",
       "  712,\n",
       "  714,\n",
       "  716,\n",
       "  718,\n",
       "  720,\n",
       "  722,\n",
       "  724,\n",
       "  726,\n",
       "  728,\n",
       "  730,\n",
       "  732,\n",
       "  734,\n",
       "  736,\n",
       "  738,\n",
       "  740,\n",
       "  742,\n",
       "  744,\n",
       "  746,\n",
       "  748,\n",
       "  750,\n",
       "  752,\n",
       "  754,\n",
       "  756,\n",
       "  758,\n",
       "  760,\n",
       "  762,\n",
       "  764,\n",
       "  766,\n",
       "  768,\n",
       "  770,\n",
       "  772,\n",
       "  774,\n",
       "  776,\n",
       "  778,\n",
       "  780,\n",
       "  782,\n",
       "  784,\n",
       "  786,\n",
       "  788,\n",
       "  790,\n",
       "  792,\n",
       "  794,\n",
       "  796,\n",
       "  798,\n",
       "  800,\n",
       "  802,\n",
       "  804,\n",
       "  806,\n",
       "  808,\n",
       "  810,\n",
       "  812,\n",
       "  814,\n",
       "  816,\n",
       "  818,\n",
       "  820,\n",
       "  822,\n",
       "  824,\n",
       "  826,\n",
       "  828,\n",
       "  830,\n",
       "  832,\n",
       "  834,\n",
       "  836,\n",
       "  838,\n",
       "  840,\n",
       "  842,\n",
       "  844,\n",
       "  846,\n",
       "  848,\n",
       "  850,\n",
       "  852,\n",
       "  854,\n",
       "  856,\n",
       "  858,\n",
       "  860,\n",
       "  862,\n",
       "  864,\n",
       "  866,\n",
       "  868,\n",
       "  870,\n",
       "  872,\n",
       "  874,\n",
       "  876,\n",
       "  878,\n",
       "  880,\n",
       "  882,\n",
       "  884,\n",
       "  886,\n",
       "  888,\n",
       "  890,\n",
       "  892,\n",
       "  894,\n",
       "  896,\n",
       "  898,\n",
       "  900,\n",
       "  902,\n",
       "  904,\n",
       "  906,\n",
       "  908,\n",
       "  910,\n",
       "  912,\n",
       "  914,\n",
       "  916,\n",
       "  918,\n",
       "  920,\n",
       "  922,\n",
       "  924,\n",
       "  926,\n",
       "  928,\n",
       "  930,\n",
       "  932,\n",
       "  934,\n",
       "  936,\n",
       "  938,\n",
       "  940,\n",
       "  942,\n",
       "  944,\n",
       "  946,\n",
       "  948,\n",
       "  950,\n",
       "  952,\n",
       "  954,\n",
       "  956,\n",
       "  958,\n",
       "  960,\n",
       "  962,\n",
       "  964,\n",
       "  966,\n",
       "  968,\n",
       "  970,\n",
       "  972,\n",
       "  974,\n",
       "  976,\n",
       "  978,\n",
       "  980,\n",
       "  982,\n",
       "  984,\n",
       "  986,\n",
       "  988,\n",
       "  990,\n",
       "  992,\n",
       "  994,\n",
       "  996,\n",
       "  998,\n",
       "  1000,\n",
       "  1002,\n",
       "  1004,\n",
       "  1006,\n",
       "  1008,\n",
       "  1010,\n",
       "  1012,\n",
       "  1014,\n",
       "  1016,\n",
       "  1018,\n",
       "  1020,\n",
       "  1022,\n",
       "  1024,\n",
       "  1026,\n",
       "  1028,\n",
       "  1030,\n",
       "  1032,\n",
       "  1034,\n",
       "  1036,\n",
       "  1038,\n",
       "  1040,\n",
       "  1042,\n",
       "  1044,\n",
       "  1046,\n",
       "  1048,\n",
       "  1050,\n",
       "  1052,\n",
       "  1054,\n",
       "  1056,\n",
       "  1058,\n",
       "  1060,\n",
       "  1062,\n",
       "  1064,\n",
       "  1066,\n",
       "  1068,\n",
       "  1070,\n",
       "  1072,\n",
       "  1074,\n",
       "  1076,\n",
       "  1078,\n",
       "  1080,\n",
       "  1082,\n",
       "  1084,\n",
       "  1086,\n",
       "  1088,\n",
       "  1090,\n",
       "  1092,\n",
       "  1094,\n",
       "  1096,\n",
       "  1098,\n",
       "  1100,\n",
       "  1102,\n",
       "  1104,\n",
       "  1106,\n",
       "  1108,\n",
       "  1110,\n",
       "  1112,\n",
       "  1114,\n",
       "  1116,\n",
       "  1118,\n",
       "  1120,\n",
       "  1122,\n",
       "  1124,\n",
       "  1126,\n",
       "  1128,\n",
       "  1130,\n",
       "  1132,\n",
       "  1134,\n",
       "  1136,\n",
       "  1138,\n",
       "  1140,\n",
       "  1142,\n",
       "  1144,\n",
       "  1146,\n",
       "  1148,\n",
       "  1150,\n",
       "  1152,\n",
       "  1154,\n",
       "  1156,\n",
       "  1158,\n",
       "  1160,\n",
       "  1162,\n",
       "  1164,\n",
       "  1166,\n",
       "  1168,\n",
       "  1170,\n",
       "  1172,\n",
       "  1174,\n",
       "  1176,\n",
       "  1178,\n",
       "  1180,\n",
       "  1182,\n",
       "  1184,\n",
       "  1186,\n",
       "  1188,\n",
       "  1190,\n",
       "  1192,\n",
       "  1194,\n",
       "  1196,\n",
       "  1198,\n",
       "  1200,\n",
       "  1202,\n",
       "  1204,\n",
       "  1206,\n",
       "  1208,\n",
       "  1210,\n",
       "  1212,\n",
       "  1214,\n",
       "  1216,\n",
       "  1218,\n",
       "  1220,\n",
       "  1222,\n",
       "  1224,\n",
       "  1226,\n",
       "  1228,\n",
       "  1230,\n",
       "  1232,\n",
       "  1234,\n",
       "  1236,\n",
       "  1238,\n",
       "  1240,\n",
       "  1242,\n",
       "  1244,\n",
       "  1246,\n",
       "  1248,\n",
       "  1250,\n",
       "  1252,\n",
       "  1254,\n",
       "  1256,\n",
       "  1258,\n",
       "  1260,\n",
       "  1262,\n",
       "  1264,\n",
       "  1266,\n",
       "  1268,\n",
       "  1270,\n",
       "  1272,\n",
       "  1274,\n",
       "  1276,\n",
       "  1278,\n",
       "  1280,\n",
       "  1282,\n",
       "  1284,\n",
       "  1286,\n",
       "  1288,\n",
       "  1290,\n",
       "  1292,\n",
       "  1294,\n",
       "  1296,\n",
       "  1298,\n",
       "  1300,\n",
       "  1302,\n",
       "  1304,\n",
       "  1306,\n",
       "  1308,\n",
       "  1310,\n",
       "  1312,\n",
       "  1314,\n",
       "  1316,\n",
       "  1318,\n",
       "  1320,\n",
       "  1322,\n",
       "  1324,\n",
       "  1326,\n",
       "  1328,\n",
       "  1330,\n",
       "  1332,\n",
       "  1334,\n",
       "  1336,\n",
       "  1338,\n",
       "  1340,\n",
       "  1342,\n",
       "  1344,\n",
       "  1346,\n",
       "  1348,\n",
       "  1350,\n",
       "  1352,\n",
       "  1354,\n",
       "  1356,\n",
       "  1358,\n",
       "  1360,\n",
       "  1362,\n",
       "  1364,\n",
       "  1366,\n",
       "  1368,\n",
       "  1370,\n",
       "  1372,\n",
       "  1374,\n",
       "  1376,\n",
       "  1378,\n",
       "  1380,\n",
       "  1382,\n",
       "  1384,\n",
       "  1386,\n",
       "  1388,\n",
       "  1390,\n",
       "  1392,\n",
       "  1394,\n",
       "  1396,\n",
       "  1398,\n",
       "  1400,\n",
       "  1402,\n",
       "  1404,\n",
       "  1406,\n",
       "  1408,\n",
       "  1410,\n",
       "  1412,\n",
       "  1414,\n",
       "  1416,\n",
       "  1418,\n",
       "  1420,\n",
       "  1422,\n",
       "  1424,\n",
       "  1426,\n",
       "  1428,\n",
       "  1430,\n",
       "  1432,\n",
       "  1434,\n",
       "  1436,\n",
       "  1438,\n",
       "  1440,\n",
       "  1442,\n",
       "  1444,\n",
       "  1446,\n",
       "  1448,\n",
       "  1450,\n",
       "  1452,\n",
       "  1454,\n",
       "  1456,\n",
       "  1458,\n",
       "  1460,\n",
       "  1462,\n",
       "  1464,\n",
       "  1466,\n",
       "  1468,\n",
       "  1470,\n",
       "  1472,\n",
       "  1474,\n",
       "  1476,\n",
       "  1478,\n",
       "  1480,\n",
       "  1482,\n",
       "  1484,\n",
       "  1486,\n",
       "  1488,\n",
       "  1490,\n",
       "  1492,\n",
       "  1494,\n",
       "  1496,\n",
       "  1498,\n",
       "  1500,\n",
       "  1502,\n",
       "  1504,\n",
       "  1506,\n",
       "  1508,\n",
       "  1510,\n",
       "  1512,\n",
       "  1514,\n",
       "  1516,\n",
       "  1518,\n",
       "  1520,\n",
       "  1522,\n",
       "  1524,\n",
       "  1526,\n",
       "  1528,\n",
       "  1530,\n",
       "  1532,\n",
       "  1534,\n",
       "  1536,\n",
       "  1538,\n",
       "  1540,\n",
       "  1542,\n",
       "  1544,\n",
       "  1546,\n",
       "  1548,\n",
       "  1550,\n",
       "  1552,\n",
       "  1554,\n",
       "  1556,\n",
       "  1558,\n",
       "  1560,\n",
       "  1562,\n",
       "  1564,\n",
       "  1566,\n",
       "  1568,\n",
       "  1570,\n",
       "  1572,\n",
       "  1574,\n",
       "  1576,\n",
       "  1578,\n",
       "  1580,\n",
       "  1582,\n",
       "  1584,\n",
       "  1586,\n",
       "  1588,\n",
       "  1590,\n",
       "  1592,\n",
       "  1594,\n",
       "  1596,\n",
       "  1598,\n",
       "  1600,\n",
       "  1602,\n",
       "  1604,\n",
       "  1606,\n",
       "  1608,\n",
       "  1610,\n",
       "  1612,\n",
       "  1614,\n",
       "  1616,\n",
       "  1618,\n",
       "  1620,\n",
       "  1622,\n",
       "  1624,\n",
       "  1626,\n",
       "  1628,\n",
       "  1630,\n",
       "  1632,\n",
       "  1634,\n",
       "  1636,\n",
       "  1638,\n",
       "  1640,\n",
       "  1642,\n",
       "  1644,\n",
       "  1646,\n",
       "  1648,\n",
       "  1650,\n",
       "  1652,\n",
       "  1654,\n",
       "  1656,\n",
       "  1658,\n",
       "  1660,\n",
       "  1662,\n",
       "  1664,\n",
       "  1666,\n",
       "  1668,\n",
       "  1670,\n",
       "  1672,\n",
       "  1674,\n",
       "  1676,\n",
       "  1678,\n",
       "  1680,\n",
       "  1682,\n",
       "  1684,\n",
       "  1686,\n",
       "  1688,\n",
       "  1690,\n",
       "  1692,\n",
       "  1694,\n",
       "  1696,\n",
       "  1698,\n",
       "  1700,\n",
       "  1702,\n",
       "  1704,\n",
       "  1706,\n",
       "  1708,\n",
       "  1710,\n",
       "  1712,\n",
       "  1714,\n",
       "  1716,\n",
       "  1718,\n",
       "  1720,\n",
       "  1722,\n",
       "  1724,\n",
       "  1726,\n",
       "  1728,\n",
       "  1730,\n",
       "  1732,\n",
       "  1734,\n",
       "  1736,\n",
       "  1738,\n",
       "  1740,\n",
       "  1742,\n",
       "  1744,\n",
       "  1746,\n",
       "  1748,\n",
       "  1750,\n",
       "  1752,\n",
       "  1754,\n",
       "  1756,\n",
       "  1758,\n",
       "  1760,\n",
       "  1762,\n",
       "  1764,\n",
       "  1766,\n",
       "  1768,\n",
       "  1770,\n",
       "  1772,\n",
       "  1774,\n",
       "  1776,\n",
       "  1778,\n",
       "  1780,\n",
       "  1782,\n",
       "  1784,\n",
       "  1786,\n",
       "  1788,\n",
       "  1790,\n",
       "  1792,\n",
       "  1794,\n",
       "  1796,\n",
       "  1798,\n",
       "  1800,\n",
       "  1802,\n",
       "  1804,\n",
       "  1806,\n",
       "  1808,\n",
       "  1810,\n",
       "  1812,\n",
       "  1814,\n",
       "  1816,\n",
       "  1818,\n",
       "  1820,\n",
       "  1822,\n",
       "  1824,\n",
       "  1826,\n",
       "  1828,\n",
       "  1830,\n",
       "  1832,\n",
       "  1834,\n",
       "  1836,\n",
       "  1838,\n",
       "  1840,\n",
       "  1842,\n",
       "  1844,\n",
       "  1846,\n",
       "  1848,\n",
       "  1850,\n",
       "  1852,\n",
       "  1854,\n",
       "  1856,\n",
       "  1858,\n",
       "  1860,\n",
       "  1862,\n",
       "  1864,\n",
       "  1866,\n",
       "  1868,\n",
       "  1870,\n",
       "  1872,\n",
       "  1874,\n",
       "  1876,\n",
       "  1878,\n",
       "  1880,\n",
       "  1882,\n",
       "  1884,\n",
       "  1886,\n",
       "  1888,\n",
       "  1890,\n",
       "  1892,\n",
       "  1894,\n",
       "  1896,\n",
       "  1898,\n",
       "  1900,\n",
       "  1902,\n",
       "  1904,\n",
       "  1906,\n",
       "  1908,\n",
       "  1910,\n",
       "  1912,\n",
       "  1914,\n",
       "  1916,\n",
       "  1918,\n",
       "  1920,\n",
       "  1922,\n",
       "  1924,\n",
       "  1926,\n",
       "  1928,\n",
       "  1930,\n",
       "  1932,\n",
       "  1934,\n",
       "  1936,\n",
       "  1938,\n",
       "  1940,\n",
       "  1942,\n",
       "  1944,\n",
       "  1946,\n",
       "  1948,\n",
       "  1950,\n",
       "  1952,\n",
       "  1954,\n",
       "  1956,\n",
       "  1958,\n",
       "  1960,\n",
       "  1962,\n",
       "  1964,\n",
       "  1966,\n",
       "  1968,\n",
       "  1970,\n",
       "  1972,\n",
       "  1974,\n",
       "  1976,\n",
       "  1978,\n",
       "  1980,\n",
       "  1982,\n",
       "  1984,\n",
       "  1986,\n",
       "  1988,\n",
       "  1990,\n",
       "  1992,\n",
       "  1994,\n",
       "  1996,\n",
       "  1998,\n",
       "  None],\n",
       " 'min_samples_split': [2, 5, 10],\n",
       " 'min_samples_leaf': [1, 2, 4],\n",
       " 'bootstrap': [True, False]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "246b9aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf is the base model\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 200, cv = 5, verbose=2, random_state=42, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c054bd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 13.9min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed: 24.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed: 36.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=RandomForestClassifier(), n_iter=200,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [100, 102, 104, 106, 108,\n",
       "                                                      110, 112, 114, 116, 118,\n",
       "                                                      120, 122, 124, 126, 128,\n",
       "                                                      130, 132, 134, 136, 138,\n",
       "                                                      140, 142, 144, 146, 148,\n",
       "                                                      150, 152, 154, 156, 158, ...],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [100, 119, 138, 157,\n",
       "                                                         176, 195, 215, 234,\n",
       "                                                         253, 272, 291, 311,\n",
       "                                                         330, 349, 368, 387,\n",
       "                                                         407, 426, 445, 464,\n",
       "                                                         483, 503, 522, 541,\n",
       "                                                         560, 579, 598, 618,\n",
       "                                                         637, 656, ...]},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12929b09",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd27ec0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, max_depth=1054, n_estimators=215)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "37c48497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best model in to instance (for save in disk)\n",
    "best_model = rf_random.best_estimator_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e4ec810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9536568117160222"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2f65e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 215,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 1054,\n",
       " 'bootstrap': False}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7eead7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=rf_random.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e02fa703",
   "metadata": {},
   "outputs": [],
   "source": [
    "err = (y_test - y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "387cb9ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0    839\n",
       " 1     77\n",
       "-1     22\n",
       "Name: classifier_val, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ffed57b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.10554371002132196\n",
      "MSE: 0.10554371002132196\n",
      "RMSE: 0.3248749144229544\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print('MAE:', metrics.mean_absolute_error(y_test,y_pred))\n",
    "print('MSE:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "10ae46b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[740  22]\n",
      " [ 77  99]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.97      0.94       762\n",
      "           1       0.82      0.56      0.67       176\n",
      "\n",
      "    accuracy                           0.89       938\n",
      "   macro avg       0.86      0.77      0.80       938\n",
      "weighted avg       0.89      0.89      0.89       938\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "499bf86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "# open a file, where you ant to store the data\n",
    "#file = open('random_forest_regression_model_v2.pkl', 'wb')\n",
    "# dump information to that file\n",
    "#pickle.dump(rf_random, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c2c90e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best score is  0.9536568117160222"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e1b741",
   "metadata": {},
   "source": [
    "### EXtra Works (just for check some doubts )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bdd30273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# again train and test by using best model we got (this is for just see the result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6e0e0d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = RandomForestClassifier(n_estimators= 215,bootstrap = False,max_depth=1054,max_features='auto',min_samples_leaf= 1,min_samples_split= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e4574f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, max_depth=1054, n_estimators=215)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e2e9e0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, max_depth=1054, n_estimators=215)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7fb189f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1aae2425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[738  24]\n",
      " [ 78  98]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.97      0.94       762\n",
      "           1       0.80      0.56      0.66       176\n",
      "\n",
      "    accuracy                           0.89       938\n",
      "   macro avg       0.85      0.76      0.80       938\n",
      "weighted avg       0.89      0.89      0.88       938\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test, y_prediction))\n",
    "print(classification_report(y_test, y_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e22cf8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base model without hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0acdd0e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_base.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f006b0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pr = model_base.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a2f3092d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[736  26]\n",
      " [ 78  98]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.97      0.93       762\n",
      "           1       0.79      0.56      0.65       176\n",
      "\n",
      "    accuracy                           0.89       938\n",
      "   macro avg       0.85      0.76      0.79       938\n",
      "weighted avg       0.88      0.89      0.88       938\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test, y_pr))\n",
    "print(classification_report(y_test, y_pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc17c44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
