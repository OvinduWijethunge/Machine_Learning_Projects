{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "315f1d18",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/OvinduWijethunge/Machine_Learning_Projects/blob/version-1/youtube%20spam%20classifier/Random%20Forest%20Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cf4fc0b",
   "metadata": {
    "id": "5cf4fc0b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50a4a8c3",
   "metadata": {
    "id": "50a4a8c3"
   },
   "outputs": [],
   "source": [
    "#url = 'https://raw.githubusercontent.com/OvinduWijethunge/Machine_Learning_Projects/version-1/youtube%20spam%20classifier/model.csv'\n",
    "#df = pd.read_csv(url)\n",
    "df = pd.read_csv('models.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa86b1ae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "aa86b1ae",
    "outputId": "d51836cf-72ed-4bc8-ea46-0c29bc89fe6e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sim_content</th>\n",
       "      <th>sim_comment</th>\n",
       "      <th>word_count</th>\n",
       "      <th>duplicate_word_ratio</th>\n",
       "      <th>length_of_comment</th>\n",
       "      <th>stop_word_ratio</th>\n",
       "      <th>post_coment_gap</th>\n",
       "      <th>black_word_count</th>\n",
       "      <th>comment_duplication</th>\n",
       "      <th>...</th>\n",
       "      <th>num_of_punctuations_1.0</th>\n",
       "      <th>num_of_punctuations_2.0</th>\n",
       "      <th>num_of_punctuations_3.0</th>\n",
       "      <th>num_of_punctuations_4.0</th>\n",
       "      <th>num_of_punctuations_5.0</th>\n",
       "      <th>num_of_punctuations_6.0</th>\n",
       "      <th>is_period_sequence_1</th>\n",
       "      <th>is_link_1</th>\n",
       "      <th>is_youtube_link_1</th>\n",
       "      <th>is_number_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.038590</td>\n",
       "      <td>-0.870409</td>\n",
       "      <td>1.866549</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.274240</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.337643</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.103307</td>\n",
       "      <td>-0.971247</td>\n",
       "      <td>1.866549</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.555200</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>29.017218</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.064844</td>\n",
       "      <td>-0.957045</td>\n",
       "      <td>2.260334</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>5.086086</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>29.017218</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.936203</td>\n",
       "      <td>-0.910982</td>\n",
       "      <td>1.683387</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.982309</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>29.017218</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.212501</td>\n",
       "      <td>-1.085031</td>\n",
       "      <td>2.122197</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.920915</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.017218</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  sim_content  sim_comment  word_count  duplicate_word_ratio  \\\n",
       "0           0    -1.038590    -0.870409    1.866549                0.0000   \n",
       "1           1    -1.103307    -0.971247    1.866549                0.0000   \n",
       "2           2    -1.064844    -0.957045    2.260334                0.0625   \n",
       "3           3    -0.936203    -0.910982    1.683387                0.0000   \n",
       "4           4    -1.212501    -1.085031    2.122197                0.0000   \n",
       "\n",
       "   length_of_comment  stop_word_ratio  post_coment_gap  black_word_count  \\\n",
       "0           4.274240         0.000000        13.337643          0.222222   \n",
       "1           4.555200         0.111111        29.017218          0.000000   \n",
       "2           5.086086         0.062500        29.017218          0.125000   \n",
       "3           3.982309         0.142857        29.017218          0.142857   \n",
       "4           4.920915         0.000000        29.017218          0.076923   \n",
       "\n",
       "   comment_duplication  ...  num_of_punctuations_1.0  num_of_punctuations_2.0  \\\n",
       "0                  0.0  ...                        0                        0   \n",
       "1                  0.0  ...                        0                        1   \n",
       "2                  0.0  ...                        0                        0   \n",
       "3                  0.0  ...                        0                        0   \n",
       "4                  0.0  ...                        0                        0   \n",
       "\n",
       "   num_of_punctuations_3.0  num_of_punctuations_4.0  num_of_punctuations_5.0  \\\n",
       "0                        0                        0                        0   \n",
       "1                        0                        0                        0   \n",
       "2                        0                        1                        0   \n",
       "3                        0                        0                        0   \n",
       "4                        0                        1                        0   \n",
       "\n",
       "   num_of_punctuations_6.0  is_period_sequence_1  is_link_1  \\\n",
       "0                        0                     0          0   \n",
       "1                        0                     0          0   \n",
       "2                        0                     0          1   \n",
       "3                        0                     0          0   \n",
       "4                        0                     1          1   \n",
       "\n",
       "   is_youtube_link_1  is_number_1  \n",
       "0                  0            0  \n",
       "1                  0            0  \n",
       "2                  1            0  \n",
       "3                  0            0  \n",
       "4                  1            0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6740b5eb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6740b5eb",
    "outputId": "a77a0d23-8fe9-430b-b118-8f0199ffd3e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'sim_content', 'sim_comment', 'word_count',\n",
       "       'duplicate_word_ratio', 'length_of_comment', 'stop_word_ratio',\n",
       "       'post_coment_gap', 'black_word_count', 'comment_duplication', 'is_spam',\n",
       "       'no_of_sentences_2', 'no_of_sentences_3', 'num_of_punctuations_1.0',\n",
       "       'num_of_punctuations_2.0', 'num_of_punctuations_3.0',\n",
       "       'num_of_punctuations_4.0', 'num_of_punctuations_5.0',\n",
       "       'num_of_punctuations_6.0', 'is_period_sequence_1', 'is_link_1',\n",
       "       'is_youtube_link_1', 'is_number_1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rename(columns={'Unnamed: 0':'id'},inplace=True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a68913c",
   "metadata": {
    "id": "5a68913c"
   },
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90728004",
   "metadata": {
    "id": "90728004"
   },
   "outputs": [],
   "source": [
    "dfc = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "799f4ace",
   "metadata": {
    "id": "799f4ace"
   },
   "outputs": [],
   "source": [
    "y = dfc['is_spam']\n",
    "X = dfc.drop(['is_spam','id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bcfb2c",
   "metadata": {
    "id": "12bcfb2c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56f77e4",
   "metadata": {
    "id": "f56f77e4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c70ae8e",
   "metadata": {
    "id": "3c70ae8e"
   },
   "source": [
    "### Handle imbalance dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "826c75aa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "826c75aa",
    "outputId": "864f9e7e-02f9-4ad4-d61a-d4c3ad547e19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    6255\n",
       "1     712\n",
       "Name: is_spam, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfc['is_spam'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a726715f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a726715f",
    "outputId": "a6c76cc1-0e34-44eb-95e4-ff3a93826068"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of classes before fit Counter({0: 6255, 1: 712})\n",
      "The number of classes after fit Counter({0: 6255, 1: 5629})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\imblearn\\utils\\_validation.py:638: FutureWarning: Pass sampling_strategy=0.9 as keyword args. From version 0.9 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "\n",
    "os=RandomOverSampler(0.9)\n",
    "X_ns,y_ns=os.fit_sample(X,y)\n",
    "print(\"The number of classes before fit {}\".format(Counter(y)))\n",
    "print(\"The number of classes after fit {}\".format(Counter(y_ns)))\n",
    "\n",
    "X = X_ns\n",
    "y = y_ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84374dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b89484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train) \n",
    "X_test = scaler.transform(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0892ced",
   "metadata": {
    "id": "f0892ced"
   },
   "source": [
    "### Hyperparameter Tuninng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f50513ee",
   "metadata": {
    "id": "f50513ee"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76118bc5",
   "metadata": {
    "id": "76118bc5"
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae1c063f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ae1c063f",
    "outputId": "ab3be14a-c958-4e2e-8f5b-a9b029a11ad5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.get_params() # hyperParameter list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "672aa74a",
   "metadata": {
    "id": "672aa74a"
   },
   "outputs": [],
   "source": [
    "# RandomizedSearchCV for find the most suited model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fbc713c",
   "metadata": {
    "id": "4fbc713c"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 100)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in range(100,2000,2)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09b09245",
   "metadata": {
    "id": "09b09245"
   },
   "outputs": [],
   "source": [
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81d80bc2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81d80bc2",
    "outputId": "c53e0e15-315b-499e-f034-85c38e8337e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': [100,\n",
       "  119,\n",
       "  138,\n",
       "  157,\n",
       "  176,\n",
       "  195,\n",
       "  215,\n",
       "  234,\n",
       "  253,\n",
       "  272,\n",
       "  291,\n",
       "  311,\n",
       "  330,\n",
       "  349,\n",
       "  368,\n",
       "  387,\n",
       "  407,\n",
       "  426,\n",
       "  445,\n",
       "  464,\n",
       "  483,\n",
       "  503,\n",
       "  522,\n",
       "  541,\n",
       "  560,\n",
       "  579,\n",
       "  598,\n",
       "  618,\n",
       "  637,\n",
       "  656,\n",
       "  675,\n",
       "  694,\n",
       "  714,\n",
       "  733,\n",
       "  752,\n",
       "  771,\n",
       "  790,\n",
       "  810,\n",
       "  829,\n",
       "  848,\n",
       "  867,\n",
       "  886,\n",
       "  906,\n",
       "  925,\n",
       "  944,\n",
       "  963,\n",
       "  982,\n",
       "  1002,\n",
       "  1021,\n",
       "  1040,\n",
       "  1059,\n",
       "  1078,\n",
       "  1097,\n",
       "  1117,\n",
       "  1136,\n",
       "  1155,\n",
       "  1174,\n",
       "  1193,\n",
       "  1213,\n",
       "  1232,\n",
       "  1251,\n",
       "  1270,\n",
       "  1289,\n",
       "  1309,\n",
       "  1328,\n",
       "  1347,\n",
       "  1366,\n",
       "  1385,\n",
       "  1405,\n",
       "  1424,\n",
       "  1443,\n",
       "  1462,\n",
       "  1481,\n",
       "  1501,\n",
       "  1520,\n",
       "  1539,\n",
       "  1558,\n",
       "  1577,\n",
       "  1596,\n",
       "  1616,\n",
       "  1635,\n",
       "  1654,\n",
       "  1673,\n",
       "  1692,\n",
       "  1712,\n",
       "  1731,\n",
       "  1750,\n",
       "  1769,\n",
       "  1788,\n",
       "  1808,\n",
       "  1827,\n",
       "  1846,\n",
       "  1865,\n",
       "  1884,\n",
       "  1904,\n",
       "  1923,\n",
       "  1942,\n",
       "  1961,\n",
       "  1980,\n",
       "  2000],\n",
       " 'max_features': ['auto', 'sqrt'],\n",
       " 'max_depth': [100,\n",
       "  102,\n",
       "  104,\n",
       "  106,\n",
       "  108,\n",
       "  110,\n",
       "  112,\n",
       "  114,\n",
       "  116,\n",
       "  118,\n",
       "  120,\n",
       "  122,\n",
       "  124,\n",
       "  126,\n",
       "  128,\n",
       "  130,\n",
       "  132,\n",
       "  134,\n",
       "  136,\n",
       "  138,\n",
       "  140,\n",
       "  142,\n",
       "  144,\n",
       "  146,\n",
       "  148,\n",
       "  150,\n",
       "  152,\n",
       "  154,\n",
       "  156,\n",
       "  158,\n",
       "  160,\n",
       "  162,\n",
       "  164,\n",
       "  166,\n",
       "  168,\n",
       "  170,\n",
       "  172,\n",
       "  174,\n",
       "  176,\n",
       "  178,\n",
       "  180,\n",
       "  182,\n",
       "  184,\n",
       "  186,\n",
       "  188,\n",
       "  190,\n",
       "  192,\n",
       "  194,\n",
       "  196,\n",
       "  198,\n",
       "  200,\n",
       "  202,\n",
       "  204,\n",
       "  206,\n",
       "  208,\n",
       "  210,\n",
       "  212,\n",
       "  214,\n",
       "  216,\n",
       "  218,\n",
       "  220,\n",
       "  222,\n",
       "  224,\n",
       "  226,\n",
       "  228,\n",
       "  230,\n",
       "  232,\n",
       "  234,\n",
       "  236,\n",
       "  238,\n",
       "  240,\n",
       "  242,\n",
       "  244,\n",
       "  246,\n",
       "  248,\n",
       "  250,\n",
       "  252,\n",
       "  254,\n",
       "  256,\n",
       "  258,\n",
       "  260,\n",
       "  262,\n",
       "  264,\n",
       "  266,\n",
       "  268,\n",
       "  270,\n",
       "  272,\n",
       "  274,\n",
       "  276,\n",
       "  278,\n",
       "  280,\n",
       "  282,\n",
       "  284,\n",
       "  286,\n",
       "  288,\n",
       "  290,\n",
       "  292,\n",
       "  294,\n",
       "  296,\n",
       "  298,\n",
       "  300,\n",
       "  302,\n",
       "  304,\n",
       "  306,\n",
       "  308,\n",
       "  310,\n",
       "  312,\n",
       "  314,\n",
       "  316,\n",
       "  318,\n",
       "  320,\n",
       "  322,\n",
       "  324,\n",
       "  326,\n",
       "  328,\n",
       "  330,\n",
       "  332,\n",
       "  334,\n",
       "  336,\n",
       "  338,\n",
       "  340,\n",
       "  342,\n",
       "  344,\n",
       "  346,\n",
       "  348,\n",
       "  350,\n",
       "  352,\n",
       "  354,\n",
       "  356,\n",
       "  358,\n",
       "  360,\n",
       "  362,\n",
       "  364,\n",
       "  366,\n",
       "  368,\n",
       "  370,\n",
       "  372,\n",
       "  374,\n",
       "  376,\n",
       "  378,\n",
       "  380,\n",
       "  382,\n",
       "  384,\n",
       "  386,\n",
       "  388,\n",
       "  390,\n",
       "  392,\n",
       "  394,\n",
       "  396,\n",
       "  398,\n",
       "  400,\n",
       "  402,\n",
       "  404,\n",
       "  406,\n",
       "  408,\n",
       "  410,\n",
       "  412,\n",
       "  414,\n",
       "  416,\n",
       "  418,\n",
       "  420,\n",
       "  422,\n",
       "  424,\n",
       "  426,\n",
       "  428,\n",
       "  430,\n",
       "  432,\n",
       "  434,\n",
       "  436,\n",
       "  438,\n",
       "  440,\n",
       "  442,\n",
       "  444,\n",
       "  446,\n",
       "  448,\n",
       "  450,\n",
       "  452,\n",
       "  454,\n",
       "  456,\n",
       "  458,\n",
       "  460,\n",
       "  462,\n",
       "  464,\n",
       "  466,\n",
       "  468,\n",
       "  470,\n",
       "  472,\n",
       "  474,\n",
       "  476,\n",
       "  478,\n",
       "  480,\n",
       "  482,\n",
       "  484,\n",
       "  486,\n",
       "  488,\n",
       "  490,\n",
       "  492,\n",
       "  494,\n",
       "  496,\n",
       "  498,\n",
       "  500,\n",
       "  502,\n",
       "  504,\n",
       "  506,\n",
       "  508,\n",
       "  510,\n",
       "  512,\n",
       "  514,\n",
       "  516,\n",
       "  518,\n",
       "  520,\n",
       "  522,\n",
       "  524,\n",
       "  526,\n",
       "  528,\n",
       "  530,\n",
       "  532,\n",
       "  534,\n",
       "  536,\n",
       "  538,\n",
       "  540,\n",
       "  542,\n",
       "  544,\n",
       "  546,\n",
       "  548,\n",
       "  550,\n",
       "  552,\n",
       "  554,\n",
       "  556,\n",
       "  558,\n",
       "  560,\n",
       "  562,\n",
       "  564,\n",
       "  566,\n",
       "  568,\n",
       "  570,\n",
       "  572,\n",
       "  574,\n",
       "  576,\n",
       "  578,\n",
       "  580,\n",
       "  582,\n",
       "  584,\n",
       "  586,\n",
       "  588,\n",
       "  590,\n",
       "  592,\n",
       "  594,\n",
       "  596,\n",
       "  598,\n",
       "  600,\n",
       "  602,\n",
       "  604,\n",
       "  606,\n",
       "  608,\n",
       "  610,\n",
       "  612,\n",
       "  614,\n",
       "  616,\n",
       "  618,\n",
       "  620,\n",
       "  622,\n",
       "  624,\n",
       "  626,\n",
       "  628,\n",
       "  630,\n",
       "  632,\n",
       "  634,\n",
       "  636,\n",
       "  638,\n",
       "  640,\n",
       "  642,\n",
       "  644,\n",
       "  646,\n",
       "  648,\n",
       "  650,\n",
       "  652,\n",
       "  654,\n",
       "  656,\n",
       "  658,\n",
       "  660,\n",
       "  662,\n",
       "  664,\n",
       "  666,\n",
       "  668,\n",
       "  670,\n",
       "  672,\n",
       "  674,\n",
       "  676,\n",
       "  678,\n",
       "  680,\n",
       "  682,\n",
       "  684,\n",
       "  686,\n",
       "  688,\n",
       "  690,\n",
       "  692,\n",
       "  694,\n",
       "  696,\n",
       "  698,\n",
       "  700,\n",
       "  702,\n",
       "  704,\n",
       "  706,\n",
       "  708,\n",
       "  710,\n",
       "  712,\n",
       "  714,\n",
       "  716,\n",
       "  718,\n",
       "  720,\n",
       "  722,\n",
       "  724,\n",
       "  726,\n",
       "  728,\n",
       "  730,\n",
       "  732,\n",
       "  734,\n",
       "  736,\n",
       "  738,\n",
       "  740,\n",
       "  742,\n",
       "  744,\n",
       "  746,\n",
       "  748,\n",
       "  750,\n",
       "  752,\n",
       "  754,\n",
       "  756,\n",
       "  758,\n",
       "  760,\n",
       "  762,\n",
       "  764,\n",
       "  766,\n",
       "  768,\n",
       "  770,\n",
       "  772,\n",
       "  774,\n",
       "  776,\n",
       "  778,\n",
       "  780,\n",
       "  782,\n",
       "  784,\n",
       "  786,\n",
       "  788,\n",
       "  790,\n",
       "  792,\n",
       "  794,\n",
       "  796,\n",
       "  798,\n",
       "  800,\n",
       "  802,\n",
       "  804,\n",
       "  806,\n",
       "  808,\n",
       "  810,\n",
       "  812,\n",
       "  814,\n",
       "  816,\n",
       "  818,\n",
       "  820,\n",
       "  822,\n",
       "  824,\n",
       "  826,\n",
       "  828,\n",
       "  830,\n",
       "  832,\n",
       "  834,\n",
       "  836,\n",
       "  838,\n",
       "  840,\n",
       "  842,\n",
       "  844,\n",
       "  846,\n",
       "  848,\n",
       "  850,\n",
       "  852,\n",
       "  854,\n",
       "  856,\n",
       "  858,\n",
       "  860,\n",
       "  862,\n",
       "  864,\n",
       "  866,\n",
       "  868,\n",
       "  870,\n",
       "  872,\n",
       "  874,\n",
       "  876,\n",
       "  878,\n",
       "  880,\n",
       "  882,\n",
       "  884,\n",
       "  886,\n",
       "  888,\n",
       "  890,\n",
       "  892,\n",
       "  894,\n",
       "  896,\n",
       "  898,\n",
       "  900,\n",
       "  902,\n",
       "  904,\n",
       "  906,\n",
       "  908,\n",
       "  910,\n",
       "  912,\n",
       "  914,\n",
       "  916,\n",
       "  918,\n",
       "  920,\n",
       "  922,\n",
       "  924,\n",
       "  926,\n",
       "  928,\n",
       "  930,\n",
       "  932,\n",
       "  934,\n",
       "  936,\n",
       "  938,\n",
       "  940,\n",
       "  942,\n",
       "  944,\n",
       "  946,\n",
       "  948,\n",
       "  950,\n",
       "  952,\n",
       "  954,\n",
       "  956,\n",
       "  958,\n",
       "  960,\n",
       "  962,\n",
       "  964,\n",
       "  966,\n",
       "  968,\n",
       "  970,\n",
       "  972,\n",
       "  974,\n",
       "  976,\n",
       "  978,\n",
       "  980,\n",
       "  982,\n",
       "  984,\n",
       "  986,\n",
       "  988,\n",
       "  990,\n",
       "  992,\n",
       "  994,\n",
       "  996,\n",
       "  998,\n",
       "  1000,\n",
       "  1002,\n",
       "  1004,\n",
       "  1006,\n",
       "  1008,\n",
       "  1010,\n",
       "  1012,\n",
       "  1014,\n",
       "  1016,\n",
       "  1018,\n",
       "  1020,\n",
       "  1022,\n",
       "  1024,\n",
       "  1026,\n",
       "  1028,\n",
       "  1030,\n",
       "  1032,\n",
       "  1034,\n",
       "  1036,\n",
       "  1038,\n",
       "  1040,\n",
       "  1042,\n",
       "  1044,\n",
       "  1046,\n",
       "  1048,\n",
       "  1050,\n",
       "  1052,\n",
       "  1054,\n",
       "  1056,\n",
       "  1058,\n",
       "  1060,\n",
       "  1062,\n",
       "  1064,\n",
       "  1066,\n",
       "  1068,\n",
       "  1070,\n",
       "  1072,\n",
       "  1074,\n",
       "  1076,\n",
       "  1078,\n",
       "  1080,\n",
       "  1082,\n",
       "  1084,\n",
       "  1086,\n",
       "  1088,\n",
       "  1090,\n",
       "  1092,\n",
       "  1094,\n",
       "  1096,\n",
       "  1098,\n",
       "  1100,\n",
       "  1102,\n",
       "  1104,\n",
       "  1106,\n",
       "  1108,\n",
       "  1110,\n",
       "  1112,\n",
       "  1114,\n",
       "  1116,\n",
       "  1118,\n",
       "  1120,\n",
       "  1122,\n",
       "  1124,\n",
       "  1126,\n",
       "  1128,\n",
       "  1130,\n",
       "  1132,\n",
       "  1134,\n",
       "  1136,\n",
       "  1138,\n",
       "  1140,\n",
       "  1142,\n",
       "  1144,\n",
       "  1146,\n",
       "  1148,\n",
       "  1150,\n",
       "  1152,\n",
       "  1154,\n",
       "  1156,\n",
       "  1158,\n",
       "  1160,\n",
       "  1162,\n",
       "  1164,\n",
       "  1166,\n",
       "  1168,\n",
       "  1170,\n",
       "  1172,\n",
       "  1174,\n",
       "  1176,\n",
       "  1178,\n",
       "  1180,\n",
       "  1182,\n",
       "  1184,\n",
       "  1186,\n",
       "  1188,\n",
       "  1190,\n",
       "  1192,\n",
       "  1194,\n",
       "  1196,\n",
       "  1198,\n",
       "  1200,\n",
       "  1202,\n",
       "  1204,\n",
       "  1206,\n",
       "  1208,\n",
       "  1210,\n",
       "  1212,\n",
       "  1214,\n",
       "  1216,\n",
       "  1218,\n",
       "  1220,\n",
       "  1222,\n",
       "  1224,\n",
       "  1226,\n",
       "  1228,\n",
       "  1230,\n",
       "  1232,\n",
       "  1234,\n",
       "  1236,\n",
       "  1238,\n",
       "  1240,\n",
       "  1242,\n",
       "  1244,\n",
       "  1246,\n",
       "  1248,\n",
       "  1250,\n",
       "  1252,\n",
       "  1254,\n",
       "  1256,\n",
       "  1258,\n",
       "  1260,\n",
       "  1262,\n",
       "  1264,\n",
       "  1266,\n",
       "  1268,\n",
       "  1270,\n",
       "  1272,\n",
       "  1274,\n",
       "  1276,\n",
       "  1278,\n",
       "  1280,\n",
       "  1282,\n",
       "  1284,\n",
       "  1286,\n",
       "  1288,\n",
       "  1290,\n",
       "  1292,\n",
       "  1294,\n",
       "  1296,\n",
       "  1298,\n",
       "  1300,\n",
       "  1302,\n",
       "  1304,\n",
       "  1306,\n",
       "  1308,\n",
       "  1310,\n",
       "  1312,\n",
       "  1314,\n",
       "  1316,\n",
       "  1318,\n",
       "  1320,\n",
       "  1322,\n",
       "  1324,\n",
       "  1326,\n",
       "  1328,\n",
       "  1330,\n",
       "  1332,\n",
       "  1334,\n",
       "  1336,\n",
       "  1338,\n",
       "  1340,\n",
       "  1342,\n",
       "  1344,\n",
       "  1346,\n",
       "  1348,\n",
       "  1350,\n",
       "  1352,\n",
       "  1354,\n",
       "  1356,\n",
       "  1358,\n",
       "  1360,\n",
       "  1362,\n",
       "  1364,\n",
       "  1366,\n",
       "  1368,\n",
       "  1370,\n",
       "  1372,\n",
       "  1374,\n",
       "  1376,\n",
       "  1378,\n",
       "  1380,\n",
       "  1382,\n",
       "  1384,\n",
       "  1386,\n",
       "  1388,\n",
       "  1390,\n",
       "  1392,\n",
       "  1394,\n",
       "  1396,\n",
       "  1398,\n",
       "  1400,\n",
       "  1402,\n",
       "  1404,\n",
       "  1406,\n",
       "  1408,\n",
       "  1410,\n",
       "  1412,\n",
       "  1414,\n",
       "  1416,\n",
       "  1418,\n",
       "  1420,\n",
       "  1422,\n",
       "  1424,\n",
       "  1426,\n",
       "  1428,\n",
       "  1430,\n",
       "  1432,\n",
       "  1434,\n",
       "  1436,\n",
       "  1438,\n",
       "  1440,\n",
       "  1442,\n",
       "  1444,\n",
       "  1446,\n",
       "  1448,\n",
       "  1450,\n",
       "  1452,\n",
       "  1454,\n",
       "  1456,\n",
       "  1458,\n",
       "  1460,\n",
       "  1462,\n",
       "  1464,\n",
       "  1466,\n",
       "  1468,\n",
       "  1470,\n",
       "  1472,\n",
       "  1474,\n",
       "  1476,\n",
       "  1478,\n",
       "  1480,\n",
       "  1482,\n",
       "  1484,\n",
       "  1486,\n",
       "  1488,\n",
       "  1490,\n",
       "  1492,\n",
       "  1494,\n",
       "  1496,\n",
       "  1498,\n",
       "  1500,\n",
       "  1502,\n",
       "  1504,\n",
       "  1506,\n",
       "  1508,\n",
       "  1510,\n",
       "  1512,\n",
       "  1514,\n",
       "  1516,\n",
       "  1518,\n",
       "  1520,\n",
       "  1522,\n",
       "  1524,\n",
       "  1526,\n",
       "  1528,\n",
       "  1530,\n",
       "  1532,\n",
       "  1534,\n",
       "  1536,\n",
       "  1538,\n",
       "  1540,\n",
       "  1542,\n",
       "  1544,\n",
       "  1546,\n",
       "  1548,\n",
       "  1550,\n",
       "  1552,\n",
       "  1554,\n",
       "  1556,\n",
       "  1558,\n",
       "  1560,\n",
       "  1562,\n",
       "  1564,\n",
       "  1566,\n",
       "  1568,\n",
       "  1570,\n",
       "  1572,\n",
       "  1574,\n",
       "  1576,\n",
       "  1578,\n",
       "  1580,\n",
       "  1582,\n",
       "  1584,\n",
       "  1586,\n",
       "  1588,\n",
       "  1590,\n",
       "  1592,\n",
       "  1594,\n",
       "  1596,\n",
       "  1598,\n",
       "  1600,\n",
       "  1602,\n",
       "  1604,\n",
       "  1606,\n",
       "  1608,\n",
       "  1610,\n",
       "  1612,\n",
       "  1614,\n",
       "  1616,\n",
       "  1618,\n",
       "  1620,\n",
       "  1622,\n",
       "  1624,\n",
       "  1626,\n",
       "  1628,\n",
       "  1630,\n",
       "  1632,\n",
       "  1634,\n",
       "  1636,\n",
       "  1638,\n",
       "  1640,\n",
       "  1642,\n",
       "  1644,\n",
       "  1646,\n",
       "  1648,\n",
       "  1650,\n",
       "  1652,\n",
       "  1654,\n",
       "  1656,\n",
       "  1658,\n",
       "  1660,\n",
       "  1662,\n",
       "  1664,\n",
       "  1666,\n",
       "  1668,\n",
       "  1670,\n",
       "  1672,\n",
       "  1674,\n",
       "  1676,\n",
       "  1678,\n",
       "  1680,\n",
       "  1682,\n",
       "  1684,\n",
       "  1686,\n",
       "  1688,\n",
       "  1690,\n",
       "  1692,\n",
       "  1694,\n",
       "  1696,\n",
       "  1698,\n",
       "  1700,\n",
       "  1702,\n",
       "  1704,\n",
       "  1706,\n",
       "  1708,\n",
       "  1710,\n",
       "  1712,\n",
       "  1714,\n",
       "  1716,\n",
       "  1718,\n",
       "  1720,\n",
       "  1722,\n",
       "  1724,\n",
       "  1726,\n",
       "  1728,\n",
       "  1730,\n",
       "  1732,\n",
       "  1734,\n",
       "  1736,\n",
       "  1738,\n",
       "  1740,\n",
       "  1742,\n",
       "  1744,\n",
       "  1746,\n",
       "  1748,\n",
       "  1750,\n",
       "  1752,\n",
       "  1754,\n",
       "  1756,\n",
       "  1758,\n",
       "  1760,\n",
       "  1762,\n",
       "  1764,\n",
       "  1766,\n",
       "  1768,\n",
       "  1770,\n",
       "  1772,\n",
       "  1774,\n",
       "  1776,\n",
       "  1778,\n",
       "  1780,\n",
       "  1782,\n",
       "  1784,\n",
       "  1786,\n",
       "  1788,\n",
       "  1790,\n",
       "  1792,\n",
       "  1794,\n",
       "  1796,\n",
       "  1798,\n",
       "  1800,\n",
       "  1802,\n",
       "  1804,\n",
       "  1806,\n",
       "  1808,\n",
       "  1810,\n",
       "  1812,\n",
       "  1814,\n",
       "  1816,\n",
       "  1818,\n",
       "  1820,\n",
       "  1822,\n",
       "  1824,\n",
       "  1826,\n",
       "  1828,\n",
       "  1830,\n",
       "  1832,\n",
       "  1834,\n",
       "  1836,\n",
       "  1838,\n",
       "  1840,\n",
       "  1842,\n",
       "  1844,\n",
       "  1846,\n",
       "  1848,\n",
       "  1850,\n",
       "  1852,\n",
       "  1854,\n",
       "  1856,\n",
       "  1858,\n",
       "  1860,\n",
       "  1862,\n",
       "  1864,\n",
       "  1866,\n",
       "  1868,\n",
       "  1870,\n",
       "  1872,\n",
       "  1874,\n",
       "  1876,\n",
       "  1878,\n",
       "  1880,\n",
       "  1882,\n",
       "  1884,\n",
       "  1886,\n",
       "  1888,\n",
       "  1890,\n",
       "  1892,\n",
       "  1894,\n",
       "  1896,\n",
       "  1898,\n",
       "  1900,\n",
       "  1902,\n",
       "  1904,\n",
       "  1906,\n",
       "  1908,\n",
       "  1910,\n",
       "  1912,\n",
       "  1914,\n",
       "  1916,\n",
       "  1918,\n",
       "  1920,\n",
       "  1922,\n",
       "  1924,\n",
       "  1926,\n",
       "  1928,\n",
       "  1930,\n",
       "  1932,\n",
       "  1934,\n",
       "  1936,\n",
       "  1938,\n",
       "  1940,\n",
       "  1942,\n",
       "  1944,\n",
       "  1946,\n",
       "  1948,\n",
       "  1950,\n",
       "  1952,\n",
       "  1954,\n",
       "  1956,\n",
       "  1958,\n",
       "  1960,\n",
       "  1962,\n",
       "  1964,\n",
       "  1966,\n",
       "  1968,\n",
       "  1970,\n",
       "  1972,\n",
       "  1974,\n",
       "  1976,\n",
       "  1978,\n",
       "  1980,\n",
       "  1982,\n",
       "  1984,\n",
       "  1986,\n",
       "  1988,\n",
       "  1990,\n",
       "  1992,\n",
       "  1994,\n",
       "  1996,\n",
       "  1998,\n",
       "  None],\n",
       " 'min_samples_split': [2, 5, 10],\n",
       " 'min_samples_leaf': [1, 2, 4],\n",
       " 'bootstrap': [True, False]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "246b9aa1",
   "metadata": {
    "id": "246b9aa1"
   },
   "outputs": [],
   "source": [
    "# rf is the base model\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 75, cv = 5, verbose=2, random_state=42, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c054bd9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9c054bd9",
    "outputId": "36b0dc24-2e78-43bd-affe-71e592345221",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 75 candidates, totalling 375 fits\n"
     ]
    }
   ],
   "source": [
    "rf_random.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12929b09",
   "metadata": {
    "id": "12929b09"
   },
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd27ec0b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dd27ec0b",
    "outputId": "fc20c38d-3af3-4eda-c849-09beb875924a"
   },
   "outputs": [],
   "source": [
    "rf_random.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c48497",
   "metadata": {
    "id": "37c48497"
   },
   "outputs": [],
   "source": [
    "# get the best model in to instance (for save in disk)\n",
    "best_model = rf_random.best_estimator_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4ec810",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8e4ec810",
    "outputId": "168c4199-0c5a-4222-b3cd-3bb4ffc77888"
   },
   "outputs": [],
   "source": [
    "rf_random.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f65e8c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b2f65e8c",
    "outputId": "bce477c7-d61d-4b46-a102-f6684778f50b"
   },
   "outputs": [],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eead7a",
   "metadata": {
    "id": "b7eead7a"
   },
   "outputs": [],
   "source": [
    "y_pred=rf_random.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02fa703",
   "metadata": {
    "id": "e02fa703"
   },
   "outputs": [],
   "source": [
    "err = (y_test - y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387cb9ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "387cb9ec",
    "outputId": "9a847fa4-692a-440e-b248-278b9da66154"
   },
   "outputs": [],
   "source": [
    "err.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c68677",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "b0c68677",
    "outputId": "3c0df855-76dd-4404-a13f-241b419d33a6"
   },
   "outputs": [],
   "source": [
    "err.value_counts().plot(kind='bar',color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffed57b6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ffed57b6",
    "outputId": "a5a00d53-35b0-483f-b45b-8eca902a2b73"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print('MAE:', metrics.mean_absolute_error(y_test,y_pred))\n",
    "print('MSE:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f721738b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "f721738b",
    "outputId": "d5b38faf-d2f8-48da-88e8-350bcf0b0ef9"
   },
   "outputs": [],
   "source": [
    "indices=list(X_train)\n",
    "feat_imp = pd.Series(best_model.feature_importances_, indices).sort_values(ascending=False)\n",
    "feat_imp.plot(kind='bar', title='Importance of Features')\n",
    "plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ae46b5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "10ae46b5",
    "outputId": "acacb968-099c-47a4-d921-82b29ba4e462"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e1b741",
   "metadata": {
    "id": "a3e1b741"
   },
   "source": [
    "### ROC & AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc17c44",
   "metadata": {
    "id": "8fc17c44"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31ff193",
   "metadata": {
    "id": "b31ff193"
   },
   "outputs": [],
   "source": [
    "baseline_roc_auc = roc_auc_score(y_test, best_model.predict(X_test))\n",
    "fprB, tprB, thresholdsB = roc_curve(y_test, best_model.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4731c15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "d4731c15",
    "outputId": "a4af8907-bf78-4bf0-b107-6e3be1cd3673"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(fprB, tprB, label='GB(area = %0.2f)' % baseline_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd1f0f2",
   "metadata": {
    "id": "5dd1f0f2"
   },
   "source": [
    "### Recall - presicion with threshold values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0e0d46",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "6e0e0d46",
    "outputId": "0adf0966-08f1-4d3e-815d-dff9abe8a1b7"
   },
   "outputs": [],
   "source": [
    "### Predict test_y values and probabilities based on random forest model\n",
    "probs_y=best_model.predict_proba(X_test) \n",
    "  # probs_y is a 2-D array of probability of being labeled as 0 (first column of array) vs 1 (2nd column in array)\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, probs_y[:, 1]) \n",
    "   #retrieve probability of being 1(in second column of probs_y)\n",
    "pr_auc = metrics.auc(recall, precision)\n",
    "\n",
    "plt.title(\"Precision-Recall vs Threshold Chart\")\n",
    "plt.plot(thresholds, precision[: -1], \"b--\", label=\"Precision\")\n",
    "plt.plot(thresholds, recall[: -1], \"r--\", label=\"Recall\")\n",
    "plt.ylabel(\"Precision, Recall\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "ffplt.ylim([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64b34e0",
   "metadata": {
    "id": "b64b34e0"
   },
   "source": [
    "### After evaluate your best model among from all models then you can change the threshold of chosen model as your requirements as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e9e0c8",
   "metadata": {
    "id": "e2e9e0c8"
   },
   "outputs": [],
   "source": [
    "#THRESHOLD = 0.6\n",
    "#y_pred = np.where(best_model.predict_proba(X_test.values)[:,1] > THRESHOLD, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb189f3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7fb189f3",
    "outputId": "2c744738-da94-4caf-f140-40ec6518ba57"
   },
   "outputs": [],
   "source": [
    "#from sklearn.metrics import classification_report, confusion_matrix\n",
    "#print(confusion_matrix(y_test, y_pred))\n",
    "#print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aae2425",
   "metadata": {
    "id": "1aae2425"
   },
   "outputs": [],
   "source": [
    "# we should choose the treshold according to our reqiurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22cf8e7",
   "metadata": {
    "id": "e22cf8e7"
   },
   "outputs": [],
   "source": [
    "# when you go to production mode,\n",
    "# you should save the model\n",
    "# you should save the scaler object\n",
    "# should add your custom threshold value when you predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78005f57",
   "metadata": {
    "id": "78005f57"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "#open a file, where you ant to store the data\n",
    "file = open('random_forest_regression_modelv1.pkl', 'wb')\n",
    "# dump information to that file\n",
    "pickle.dump(best_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acdd0e2",
   "metadata": {
    "id": "0acdd0e2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JfUVb6YM2oq1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JfUVb6YM2oq1",
    "outputId": "acc72cae-eb78-4889-eff9-ed3b6c52b470"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f006b0a3",
   "metadata": {
    "id": "f006b0a3"
   },
   "outputs": [],
   "source": [
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# 1. Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)  \n",
    "\n",
    "# get the folder id where you want to save your file\n",
    "#file = drive.CreateFile({'parents':[{u'id': \"https://drive.google.com/drive/u/0/folders/1bgPwYjW2Sz32mFcc7XLfr49Icv9xzW0B\"}]})\n",
    "#file.SetContentFile('random_forest_regression_model.pkl')\n",
    "#file.Upload() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e14292",
   "metadata": {
    "id": "92e14292"
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.listdir('.')[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QdDQoXcC5qDo",
   "metadata": {
    "id": "QdDQoXcC5qDo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "Random Forest Model.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
